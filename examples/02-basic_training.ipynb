{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71ca3ef",
   "metadata": {},
   "source": [
    "# Basic training and fine-tuning with RAGatouille\n",
    "\n",
    "In this quick example, we'll use the `RAGTrainer` magic class to demonstrate how to very easily fine-tune an existing ColBERT model, or train one from any BERT/RoBERTa-like model (to [train one for a previously unsupported language like Japanese](https://huggingface.co/bclavie/jacolbert), for example!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f69fca",
   "metadata": {},
   "source": [
    "First, we'll create an instance of `RAGtrainer`. We need to give it two arguments: the `model_name` we want to give to the model we're training, and the `pretrained_model_name` of the base model. This can either be a local path, or the name of a model on the HuggingFace Hub. If you're training for a language other than English, you should also include a `language_code` two-letter argument (PLACEHOLDER ISO) so we can get the relevant processing utils!\n",
    "\n",
    "The trainer will auto-detect whether it's an existing ColBERT model or a BERT base model, and will set itself up accordingly!\n",
    "\n",
    "Please note: Training can currently only be ran on GPU, and will error out if using CPU/MPS! Training is also currently not functional on Google Colab and Windows 10.\n",
    "\n",
    "Whether we're training from scratch or fine-tuning doesn't matter, all the steps are the same. For this example, let's fine-tune ColBERTv2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81ac64-d222-412c-925c-2a5262266c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGTrainer\n",
    "trainer = RAGTrainer(model_name=\"GhibliColBERT\", pretrained_model_name=\"colbert-ir/colbertv2.0\", language_code=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8016a",
   "metadata": {},
   "source": [
    "To train retrieval models like colberts, we need training triplets: queries, positive passages, and negative passages for each query.\n",
    "\n",
    "In the next tutorial, we'll see [how to generate synthetic queries when we don't have any annotated passage]. For this tutorial, we'll assume that we have queries and relevant passages, but that we're lacking negative ones (because it's not an information we gather from our users).\n",
    "\n",
    "Let's assume our corpus is the same as the one we [used for our example about indexing an searching](PLACEHOLDER): Hayao Miyazaki's wikipedia page. Let's first fetch the content from Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca72a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "    \n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\n",
    "        \"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data['query']['pages'].values()))\n",
    "    return page['extract'] if 'extract' in page else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8ade7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_full_corpus = [get_wikipedia_page(\"Hayao_Miyazaki\")]\n",
    "my_full_corpus += [get_wikipedia_page(\"Studio_Ghibli\")]\n",
    "my_full_corpus += [get_wikipedia_page(\"Toei_Animation\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31778aed",
   "metadata": {},
   "source": [
    "We're also some Toei Animation content -- it helps to have things in our corpus that aren't directly relevant to our queries but are likely to cover similar topics, so we can get some more adjacent negative examples.\n",
    "\n",
    "The documents are very long, so let's use a `CorpusProcessor` to split them into chunks of around 256 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a14fe476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragatouille.data import CorpusProcessor, llama_index_sentence_splitter\n",
    "\n",
    "corpus_processor = CorpusProcessor(document_splitter_fn=llama_index_sentence_splitter)\n",
    "documents = corpus_processor.process_corpus(my_full_corpus, chunk_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26eea4c",
   "metadata": {},
   "source": [
    "Now that we have a corpus of documents, let's generate fake query-relevant passage pair. Obviously, you wouldn't want that in the real world, but that's the topic of the next tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b48c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "queries = [\"What manga did Hayao Miyazaki write?\",\n",
    "           \"which film made ghibli famous internationally\",\n",
    "           \"who directed Spirited Away?\",\n",
    "           \"when was Hikotei Jidai published?\",\n",
    "           \"where's studio ghibli based?\",\n",
    "           \"where is the ghibli museum?\"\n",
    "] * 3\n",
    "pairs = []\n",
    "\n",
    "for query in queries:\n",
    "    fake_relevant_docs = random.sample(documents, 10)\n",
    "    for doc in fake_relevant_docs:\n",
    "        pairs.append((query, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f29b15",
   "metadata": {},
   "source": [
    "Here, we have created pairs.It's common for retrieval training data to be stored in a lot of different ways: pairs of [query, positive], pairs of [query, passage, label], triplets of [query, positive, negative], or triplets of [query, list_of_positives, list_of_negatives]. No matter which format your data's in, you don't need to worry about it: RAGatouille will generate ColBERT-friendly triplets for you, and export them to disk for easy `dvc` or `wandb` data tracking.\n",
    "\n",
    "Speaking of, let's process the data so it's ready for training. `RAGTrainer` has a `prepare_training_data` function, which will perform all the necessary steps. One of the steps it performs is called **hard negative mining**: that's searching the full collection of documents (even those not linked to a query) to retrieve passages that are semantically close to a query, but aren't actually relevant. Using those to train retrieval models has repeatedly been shown to greatly improve their ability to find actually relevant documents, so it's a very important step! \n",
    "\n",
    "RAGatouille handles all of this for you. By default, it'll fetch 10 negative examples per query, but you can customise this with `num_new_negatives`. You can also choose not to mine negatives and just sample random examples to speed up things, this might lower performance but will run done much quicker on large volumes of data, just set `mine_hard_negatives` to `False`. If you've already mined negatives yourself, you can set `num_new_negatives` to 0 to bypass this entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddaf3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.prepare_training_data(raw_data=pairs, data_out_path=\"./data/\", all_documents=documents, num_new_negatives=10, mine_hard_negatives=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9468e25",
   "metadata": {},
   "source": [
    "Our training data's now fully processed and saved to disk in `data_out_path`! We're now ready to begin training our model with the `train` function. `train` takes many arguments, but the set of default is already fairly strong!\n",
    "\n",
    "Don't be surprised you don't see an `epochs` parameter here, ColBERT will train until it either reaches `maxsteps` or has seen the entire training data once (a full epoch), this is by design!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06773f75-c844-42a4-b786-e56ef899a96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer.train(batch_size=32,\n",
    "              nbits=4, # How many bits will the trained model use when compressing indexes\n",
    "              maxsteps=500000, # Maximum steps hard stop\n",
    "              use_ib_negatives=True, # Use in-batch negative to calculate loss\n",
    "              dim=128, # How many dimensions per embedding. 128 is the default and works well.\n",
    "              learning_rate=5e-6, # Learning rate, small values ([3e-6,3e-5] work best if the base model is BERT-like, 5e-6 is often the sweet spot)\n",
    "              doc_maxlen=256, # Maximum document length. Because of how ColBERT works, smaller chunks (128-256) work very well.\n",
    "              use_relu=False, # Disable ReLU -- doesn't improve performance\n",
    "              warmup_steps=\"auto\", # Defaults to 10%\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab6a51-a4bd-4fab-96d7-dd8e93dac462",
   "metadata": {},
   "source": [
    "And you're now done training! Your model is saved at the path it outputs, with the final checkpoint always being in the `.../checkpoints/colbert` path, and intermediate checkpoints saved at `.../checkpoints/colbert-{N_STEPS}`.\n",
    "\n",
    "You can now use your model by pointing at its local path, or upload it to the huggingface hub to share it with the world!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
